{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c547ea6",
   "metadata": {},
   "source": [
    "# Business Recorder Webscraping Project\n",
    "\n",
    "Author: Muhammad Fouzan Akhter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a090609",
   "metadata": {},
   "source": [
    "The code for a web scraping project that targets Business Recorder is shown below. Underscoring the importance of following website privacy policies is crucial for any online scraping project. It is imperative to highlight that this project is scraping entirely publicly accessible data from Yahoo Finance while adhering to the platform's privacy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing required packages:\n",
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347133d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b44228",
   "metadata": {},
   "source": [
    "**This Project is coded in the Jupyter Notebook Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c935f",
   "metadata": {},
   "source": [
    "The loops below launch a web driver and retrieve data from the designated website, Business Recorder. For every article, it extracts the header, the datetime, the content type, the content, and the link. Until the maximum scroll is achieved, the code automatically scrolls down. A pandas data frameÂ contains all of the extracted data. Collected links whose data could not be recovered because of error 403 are shown by the end of the code if you chose to display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.brecorder.com/business-finance'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #increase sleep time if internet connection is weak:\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "page_source = driver.page_source\n",
    "driver.quit()\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "links = [a['href'] for a in soup.find_all('a', class_='story__link')]\n",
    "unique_links = list(set(links))\n",
    "df_list = []\n",
    "skipped_links_403 = []\n",
    "for link in tqdm(unique_links, desc=\"Scraping Articles\"):\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        heading_element = soup.find('a', class_='story__link')\n",
    "        heading = heading_element.text.strip() if heading_element else None\n",
    "        content_type_element = soup.find('span', class_='badge')\n",
    "        content_type = content_type_element.text.strip() if content_type_element else None\n",
    "        datetime_element = soup.find('span', class_='timestamp--date')\n",
    "        datetime = datetime_element.text.strip() if datetime_element else None\n",
    "        content_element = soup.find('div', class_='story__content')\n",
    "        content_paragraphs = content_element.find_all('p') if content_element else []\n",
    "        content = '\\n'.join(paragraph.text.strip() for paragraph in content_paragraphs)\n",
    "        df_list.append(pd.DataFrame({\n",
    "            'Heading': [heading],\n",
    "            'Datetime': [datetime],\n",
    "            'Content Type': [content_type],\n",
    "            'Content': [content]\n",
    "        }))\n",
    "    elif response.status_code == 403:\n",
    "        skipped_links_403.append(link)\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch content from {link}. Status code {response.status_code}\")\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "if skipped_links_403:\n",
    "    choice = input(f\"\\n{len(skipped_links_403)} links were skipped due to error 403. Do you want to display them? (y/n): \")\n",
    "    if choice.lower() == 'y':\n",
    "        print(\"\\nSkipped Links:\")\n",
    "        print(skipped_links_403)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3b92f",
   "metadata": {},
   "source": [
    "**Viewing Scraped Data in Python Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7071a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying scraped data in python environment:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a996ce",
   "metadata": {},
   "source": [
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
